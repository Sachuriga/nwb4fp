{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b224f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(r'Q:\\sachuriga\\Sachuriga_Python/quattrocolo-nwb4fp\\src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c13820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachur\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pickle import TRUE\n",
    "import spikeinterface as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.postprocessing as post\n",
    "from nwb4fp.postprocess.Get_positions import load_positions,load_positions_h5,test_positions_h5\n",
    "from nwb4fp.postprocess.get_potential_merge import get_potential_merge\n",
    "from spikeinterface.preprocessing import (bandpass_filter,\n",
    "                                           common_reference,\n",
    "                                           whiten)\n",
    "import spikeinterface.exporters as sex\n",
    "\n",
    "from nwb4fp.preprocess.down_sample_lfp import down_sample_lfp_test\n",
    "\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from nwb4fp.postprocess.extract_wf import wf4unim,divide_wf\n",
    "import spikeinterface.preprocessing as spre\n",
    "import numpy as np\n",
    "\n",
    "from spikeinterface.extractors.neoextractors.openephys import OpenEphysBinaryRecordingExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cb483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before mannual search the stream_name. Auto search result is Record Node 102#OE_FPGA_Acquisition_Board-101.Rhythm Data\n",
      "ASSY-236-F - cambridgeneurotech - 64ch - 6shanks\n",
      "removed ['CH12' 'CH18']\n",
      "get times for raw sortsNone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_sparsity (workers: 12 processes): 100%|██████████| 7302/7302 [00:05<00:00, 1416.48it/s]\n",
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\core\\recording_tools.py:786: UserWarning: get_noise_levels(recording, num_chunks_per_segment=20) is deprecated\n",
      "Now, you need to use get_noise_levels(recording, random_slices_kwargs=dict(num_chunks_per_segment=20, chunk_size=1000))\n",
      "Please read get_random_recording_slices() documentation for more options.\n",
      "  warnings.warn(msg)\n",
      "noise_level (workers: 12 processes): 100%|██████████| 20/20 [00:01<00:00, 10.36it/s]\n",
      "estimate_templates_with_accumulator (workers: 12 processes): 100%|██████████| 7302/7302 [00:10<00:00, 689.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing potential merge...\n",
      "\n",
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\widgets\\unit_waveforms.py:182: UserWarning: templates_percentile_shading can only be used if the 'waveforms' extension is available. Settimg templates_percentile_shading to None.\n",
      "  warn(\n",
      "estimate_sparsity (workers: 12 processes): 100%|██████████| 7302/7302 [00:04<00:00, 1691.51it/s] \n",
      "estimate_templates_with_accumulator (workers: 12 processes): 100%|██████████| 7302/7302 [00:08<00:00, 833.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing potential merge...\n",
      "\n",
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\widgets\\unit_waveforms.py:182: UserWarning: templates_percentile_shading can only be used if the 'waveforms' extension is available. Settimg templates_percentile_shading to None.\n",
      "  warn(\n",
      "estimate_sparsity (workers: 12 processes): 100%|██████████| 7302/7302 [00:04<00:00, 1630.62it/s]\n",
      "estimate_templates_with_accumulator (workers: 12 processes): 100%|██████████| 7302/7302 [00:07<00:00, 1007.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing potential merge...\n",
      "\n",
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\widgets\\unit_waveforms.py:182: UserWarning: templates_percentile_shading can only be used if the 'waveforms' extension is available. Settimg templates_percentile_shading to None.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "temp_folder = r\"C:/temp_lfp\"\n",
    "path = r\"S:\\Sachuriga\\Ephys_Recording\\CR_CA1/65091/65091_2023-08-01_15-16-29_C_phy_k_manual/\"\n",
    "memory_size = 128\n",
    "global_job_kwargs = dict(n_jobs=32, total_memory=fr\"{memory_size}G\",mp_context= \"spawn\",progress_bar=True)\n",
    "si.set_global_job_kwargs(**global_job_kwargs)\n",
    "\n",
    "sorting = se.read_phy(folder_path=path, load_all_cluster_properties=True,exclude_cluster_groups = [\"noise\", \"mua\"])\n",
    "temp_path = path.split(\"_phy\")\n",
    "raw_path = temp_path[0]\n",
    "#stream_name = 'Record Node 101#OE_FPGA_Acquisition_Board-100.Rhythm Data'\n",
    "stream_name  = OpenEphysBinaryRecordingExtractor(raw_path,stream_id='0').get_streams(raw_path)[0][0]\n",
    "print(fr\"Before mannual search the stream_name. Auto search result is {stream_name}\")\n",
    "try:\n",
    "    recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n",
    "except AssertionError:\n",
    "    try:\n",
    "        stream_name = 'Record Node 102#OE_FPGA_Acquisition_Board-101.Rhythm Data'\n",
    "        recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n",
    "    except AssertionError:\n",
    "        stream_name = 'Record Node 101#Acquisition_Board-100.Rhythm Data'\n",
    "        recording = se.read_openephys(raw_path, stream_name=stream_name, load_sync_timestamps=True)\n",
    "\n",
    "import probeinterface as pi\n",
    "\n",
    "# from probeinterface import plotting\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-236-F'\n",
    "probe = pi.get_probe(manufacturer, probe_name)\n",
    "print(probe)\n",
    "# probe.wiring_to_device('cambridgeneurotech_mini-amp-64')\n",
    "# map channels to device indices\n",
    "mapping_to_device = [\n",
    "    # connector J2 TOP\n",
    "    41, 39, 38, 37, 35, 34, 33, 32, 29, 30, 28, 26, 25, 24, 22, 20,\n",
    "    46, 45, 44, 43, 42, 40, 36, 31, 27, 23, 21, 18, 19, 17, 16, 14,\n",
    "    # connector J1 BOTTOM\n",
    "    55, 53, 54, 52, 51, 50, 49, 48, 47, 15, 13, 12, 11, 9, 10, 8,\n",
    "    63, 62, 61, 60, 59, 58, 57, 56, 7, 6, 5, 4, 3, 2, 1, 0\n",
    "]\n",
    "\n",
    "probe.set_device_channel_indices(mapping_to_device)\n",
    "probe.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"shank_ids\", \"device_channel_indices\"]]\n",
    "probegroup = pi.ProbeGroup()\n",
    "probegroup.add_probe(probe)\n",
    "\n",
    "pi.write_prb(f\"{probe_name}.prb\", probegroup, group_mode=\"by_shank\")\n",
    "recording_prb = recording.set_probe(probe, group_mode=\"by_shank\")\n",
    "rec = bandpass_filter(recording_prb, freq_min=600, freq_max=8000)\n",
    "bad_channel_ids, channel_labels = spre.detect_bad_channels(recording_prb, method='coherence+psd',n_neighbors = 11)\n",
    "print(fr\"removed {bad_channel_ids}\")\n",
    "recording_good_ch= rec.remove_channels(bad_channel_ids)\n",
    "#recording_good_channels_f = spre.interpolate_bad_channels(rec,bad_channel_ids)\n",
    "\n",
    "rec_save = common_reference(recording_good_ch, reference='global', operator='median')\n",
    "rec_w = whiten(rec_save, int_scale=200, mode='local', radius_um=100.0)\n",
    "sorting.set_property(key='group', values = sorting.get_property(\"channel_group\"))\n",
    "print(f\"get times for raw sorts{sorting.get_times()}\")\n",
    "## step to analyzer\n",
    "GLOBAL_KWARGS = dict(n_jobs=16, total_memory=fr\"{memory_size}G\", progress_bar=True, mp_context= \"spawn\", chunk_size=5000, chunk_duration=\"1s\")\n",
    "si.set_global_job_kwargs(**GLOBAL_KWARGS)\n",
    "\n",
    "analyzer = si.create_sorting_analyzer(sorting=sorting, recording=rec_w, format='memory', folder=fr\"{temp_folder}\",overwrite=True)\n",
    "we1 = analyzer.compute(\"random_spikes\",\"waveforms\")\n",
    "we1 = analyzer.compute(\"noise_levels\")\n",
    "we1 = analyzer.compute(\"templates\")\n",
    "#get potential merging sorting objects\n",
    "print(\"processing potential merge...\\n\")\n",
    "## Step to creating analyzers\n",
    "unit_locations = analyzer.compute(input=\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "unit_ids = sorting.unit_ids\n",
    "phy_TRD = Path(path + \"_manual_reports\")\n",
    "sex.export_report(sorting_analyzer = analyzer, output_folder=phy_TRD, remove_if_exists=True)\n",
    "\n",
    "\n",
    "analyzer1 = si.create_sorting_analyzer(sorting=sorting, recording=rec_w, format='memory', folder=fr\"{temp_folder}\",overwrite=True)\n",
    "analyzer1.compute(\"random_spikes\",\"waveforms\")\n",
    "analyzer1.compute(\"noise_levels\")\n",
    "analyzer1.compute(\"templates\")\n",
    "#get potential merging sorting objects\n",
    "print(\"processing potential merge...\\n\")\n",
    "## Step to creating analyzers\n",
    "unit_locations = analyzer1.compute(input=\"unit_locations\", method=\"center_of_mass\")\n",
    "unit_ids = sorting.unit_ids\n",
    "phy_TRD = Path(path + \"_manual_mass\")\n",
    "sex.export_report(sorting_analyzer = analyzer1, output_folder=phy_TRD, remove_if_exists=True)\n",
    "\n",
    "\n",
    "analyzer2 = si.create_sorting_analyzer(sorting=sorting, recording=rec_w, format='memory', folder=fr\"{temp_folder}\",overwrite=True)\n",
    "analyzer2.compute(\"random_spikes\",\"waveforms\")\n",
    "analyzer2.compute(\"noise_levels\")\n",
    "analyzer2.compute(\"templates\")\n",
    "#get potential merging sorting objects\n",
    "print(\"processing potential merge...\\n\")\n",
    "## Step to creating analyzers\n",
    "unit_locations = analyzer2.compute(input=\"unit_locations\", method=\"grid_convolution\")\n",
    "unit_ids = sorting.unit_ids\n",
    "phy_TRD = Path(path + \"_manual_reports_grid\")\n",
    "sex.export_report(sorting_analyzer = analyzer2, output_folder=phy_TRD, remove_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ae8fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging step_Before mannual search the stream_name. Auto search result is Record Node 102#OE_FPGA_Acquisition_Board-101.Rhythm Data\n",
      "LFP downsampling steps. Auto search result is Record Node 102#OE_FPGA_Acquisition_Board-101.Rhythm Data\n",
      "ASSY-236-F - cambridgeneurotech - 64ch - 6shanks\n",
      "bad channels are [11, 17]\n",
      "bad channels labels ['good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good'\n",
      " 'good' 'dead' 'good' 'good' 'good' 'good' 'good' 'dead' 'good' 'good'\n",
      " 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good'\n",
      " 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good'\n",
      " 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good'\n",
      " 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good' 'good'\n",
      " 'good' 'good' 'good' 'good']\n",
      "['CH1' 'CH2' 'CH3' 'CH4' 'CH5' 'CH6' 'CH7' 'CH8' 'CH9' 'CH10' 'CH11'\n",
      " 'CH12' 'CH13' 'CH14' 'CH15' 'CH16' 'CH17' 'CH18' 'CH19' 'CH20' 'CH21'\n",
      " 'CH22' 'CH23' 'CH24' 'CH25' 'CH26' 'CH27' 'CH28' 'CH29' 'CH30' 'CH31'\n",
      " 'CH32' 'CH33' 'CH34' 'CH35' 'CH36' 'CH37' 'CH38' 'CH39' 'CH40' 'CH41'\n",
      " 'CH42' 'CH43' 'CH44' 'CH45' 'CH46' 'CH47' 'CH48' 'CH49' 'CH50' 'CH51'\n",
      " 'CH52' 'CH53' 'CH54' 'CH55' 'CH56' 'CH57' 'CH58' 'CH59' 'CH60' 'CH61'\n",
      " 'CH62' 'CH63' 'CH64']\n",
      "processing lfp data...\n",
      "write_binary_recording \n",
      "engine=process - n_jobs=12 - samples_per_chunk=5,000 - chunk_memory=1.22 MiB - total_memory=14.65 MiB - chunk_duration=4.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording (workers: 12 processes): 100%|██████████| 305/305 [02:40<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing lfp data...\n",
      "write_binary_recording \n",
      "engine=process - n_jobs=12 - samples_per_chunk=5,000 - chunk_memory=625.00 KiB - total_memory=7.32 MiB - chunk_duration=4.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write_binary_recording (workers: 12 processes): 100%|██████████| 305/305 [00:36<00:00,  8.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from nwb4fp.preprocess.down_sample_lfp import down_sample_lfp_test\n",
    "temp_folder = r\"C:/temp_lfp\"\n",
    "path = r\"S:\\Sachuriga\\Ephys_Recording\\CR_CA1/65091/65091_2023-08-01_15-16-29_C_phy_k_manual\"\n",
    "raw_path = r\"S:\\Sachuriga\\Ephys_Recording\\CR_CA1/65091/65091_2023-08-01_15-16-29_C\"\n",
    "down_sample_lfp_test(path,raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11c0d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\widgets\\unit_waveforms.py:182: UserWarning: templates_percentile_shading can only be used if the 'waveforms' extension is available. Settimg templates_percentile_shading to None.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phy_TRD = Path(path + \"_manual_reports\")\n",
    "sex.export_report(sorting_analyzer = analyzer, output_folder=phy_TRD, remove_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73f7342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_sparsity (workers: 12 processes): 100%|██████████| 7302/7302 [00:04<00:00, 1669.72it/s]\n",
      "estimate_templates_with_accumulator (workers: 12 processes): 100%|██████████| 7302/7302 [00:07<00:00, 981.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing potential merge...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer1 = si.create_sorting_analyzer(sorting=sorting, recording=rec_w, format='memory', folder=fr\"{temp_folder}\",overwrite=True)\n",
    "analyzer1.compute(\"random_spikes\",\"waveforms\")\n",
    "analyzer1.compute(\"noise_levels\")\n",
    "analyzer1.compute(\"templates\")\n",
    "#get potential merging sorting objects\n",
    "print(\"processing potential merge...\\n\")\n",
    "## Step to creating analyzers\n",
    "unit_locations = analyzer1.compute(input=\"unit_locations\", method=\"center_of_mass\")\n",
    "unit_ids = sorting.unit_ids\n",
    "phy_TRD = Path(path + \"_manual_mass\")\n",
    "#sex.export_report(sorting_analyzer = analyzer1, output_folder=phy_TRD, remove_if_exists=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba709346",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m unit_loc_mt\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(unit_ids))\n\u001b[0;32m      3\u001b[0m unit_loc_mt\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(unit_locations\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munit_locations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munit_loc_mt.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit_loc_mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\numpy\\lib\\npyio.py:545\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m--> 545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_array(fid, arr, allow_pickle\u001b[38;5;241m=\u001b[39mallow_pickle,\n\u001b[0;32m    547\u001b[0m                        pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(fix_imports\u001b[38;5;241m=\u001b[39mfix_imports))\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "unit_loc_mt=[]\n",
    "unit_loc_mt.append(list(unit_ids))\n",
    "unit_loc_mt.append(list(unit_locations.data['unit_locations'].T))\n",
    "np.save('unit_loc_mt.npy', unit_loc_mt) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b0951a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7, 10, 11, 25, 29, 2, 3, 9, 14, 18, 20, 33, 30],\n",
       " [array([207.16263843, 202.82946004, 209.52251038, 203.74349041,\n",
       "         613.09690519, 604.14581035,  11.69277035,   5.23775735,\n",
       "         211.61895249, 407.45807776, 406.45500903, 403.39049644,\n",
       "         809.82154077, 605.02105199]),\n",
       "  array([ 47.12129809,  54.47187184,  55.11142142,  41.66844636,\n",
       "          34.90147152,  14.78394834,  32.78757262,  23.93706088,\n",
       "          54.14107789,  62.0514003 ,  33.5406549 ,  35.22035646,\n",
       "          52.89583812, 105.09188348])]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_loc_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6279e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_sparsity (workers: 12 processes): 100%|██████████| 7705/7705 [00:04<00:00, 1641.66it/s] \n",
      "estimate_templates_with_accumulator (workers: 12 processes): 100%|██████████| 7705/7705 [00:10<00:00, 765.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing potential merge...\n",
      "\n",
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'S:\\\\Sachuriga\\\\Ephys_Recording\\\\CR_CA1\\\\65588\\\\65588_2024-03-04_15-44-37_A_phy_k_manual\\\\_manual_reports_grid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m unit_ids \u001b[38;5;241m=\u001b[39m sorting\u001b[38;5;241m.\u001b[39munit_ids\n\u001b[0;32m     10\u001b[0m phy_TRD \u001b[38;5;241m=\u001b[39m Path(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_manual_reports_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43msex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorting_analyzer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43manalyzer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphy_TRD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\exporters\\report.py:98\u001b[0m, in \u001b[0;36mexport_report\u001b[1;34m(sorting_analyzer, output_folder, remove_if_exists, format, show_figures, peak_sign, force_computation, **job_kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# unit list\u001b[39;00m\n\u001b[0;32m    101\u001b[0m units \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39munit_ids)  \u001b[38;5;66;03m#  , columns=['max_on_channel_id', 'amplitude'])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'S:\\\\Sachuriga\\\\Ephys_Recording\\\\CR_CA1\\\\65588\\\\65588_2024-03-04_15-44-37_A_phy_k_manual\\\\_manual_reports_grid'"
     ]
    }
   ],
   "source": [
    "analyzer2 = si.create_sorting_analyzer(sorting=sorting, recording=rec_w, format='memory', folder=fr\"{temp_folder}\",overwrite=True)\n",
    "analyzer2.compute(\"random_spikes\",\"waveforms\")\n",
    "analyzer2.compute(\"noise_levels\")\n",
    "analyzer2.compute(\"templates\")\n",
    "#get potential merging sorting objects\n",
    "print(\"processing potential merge...\\n\")\n",
    "## Step to creating analyzers\n",
    "unit_locations = analyzer2.compute(input=\"unit_locations\", method=\"grid_convolution\")\n",
    "unit_ids = sorting.unit_ids\n",
    "phy_TRD = Path(path + \"_manual_reports_grid\")\n",
    "sex.export_report(sorting_analyzer = analyzer2, output_folder=phy_TRD, remove_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b2d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_report(): spike_amplitudes will not be exported. Use sorting_analyzer.compute('spike_amplitudes') if you want to include them.\n",
      "export_report(): quality metrics will not be exported. Use sorting_analyzer.compute('quality_metrics') if you want to include them.\n",
      "export_report(): correlograms will not be exported. Use sorting_anlyzer.compute('correlograms') if you want to include them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sachur\\AppData\\Local\\anaconda3\\envs\\test1\\Lib\\site-packages\\spikeinterface\\widgets\\unit_waveforms.py:182: UserWarning: templates_percentile_shading can only be used if the 'waveforms' extension is available. Settimg templates_percentile_shading to None.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "sex.export_report(sorting_analyzer = analyzer2, output_folder=phy_TRD, remove_if_exists=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
